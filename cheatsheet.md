# üìò Data Science Cheatsheet: Pandas, NumPy, Scikit-learn

T√†i li·ªáu t·ªïng h·ª£p c√°c c√¢u l·ªánh c·ªët l√µi th∆∞·ªùng d√πng trong quy tr√¨nh Ph√¢n t√≠ch d·ªØ li·ªáu v√† H·ªçc m√°y v·ªõi Python.

---

## 1. Pandas üêº

**M·ª•c ƒë√≠ch:** X·ª≠ l√Ω v√† ph√¢n t√≠ch d·ªØ li·ªáu d·∫°ng b·∫£ng (DataFrame).

### üõ†Ô∏è Thi·∫øt L·∫≠p & ƒê·ªçc D·ªØ Li·ªáu

| H√†nh ƒë·ªông     | C√∫ ph√°p                             | Ghi ch√∫                  |
|:--------------|:------------------------------------|:-------------------------|
| **Import**    | `import pandas as pd`               |                          |
| **ƒê·ªçc CSV**   | `df = pd.read_csv('file.csv')`      |                          |
| **ƒê·ªçc Excel** | `df = pd.read_excel('file.xlsx')`   | C·∫ßn th∆∞ vi·ªán `openpyxl`. |
| **Ghi CSV**   | `df.to_csv('out.csv', index=False)` | L∆∞u file, b·ªè c·ªôt index.  |

### üîç Kh√°m Ph√° D·ªØ Li·ªáu

| H√†nh ƒë·ªông        | C√∫ ph√°p                     | Ghi ch√∫                             |
|:-----------------|:----------------------------|:------------------------------------|
| **Xem ƒë·∫ßu/ƒëu√¥i** | `df.head(n)` / `df.tail(n)` | M·∫∑c ƒë·ªãnh n=5.                       |
| **C·∫•u tr√∫c**     | `df.info()`                 | Ki·ªÉu d·ªØ li·ªáu, b·ªô nh·ªõ, gi√° tr·ªã null. |
| **Th·ªëng k√™**     | `df.describe()`             | Mean, std, min, max (c·ªôt s·ªë).       |
| **K√≠ch th∆∞·ªõc**   | `df.shape`                  | Tr·∫£ v·ªÅ (h√†ng, c·ªôt).                 |
| **T√™n c·ªôt**      | `df.columns`                | Danh s√°ch t√™n c√°c c·ªôt.              |

### üéØ Ch·ªçn L·ªçc (Indexing & Selection)

| H√†nh ƒë·ªông          | C√∫ ph√°p                     | Ghi ch√∫                     |
|:-------------------|:----------------------------|:----------------------------|
| **Ch·ªçn c·ªôt**       | `df['col_name']`            | Tr·∫£ v·ªÅ Series.              |
| **Ch·ªçn nhi·ªÅu c·ªôt** | `df[['col1', 'col2']]`      | Tr·∫£ v·ªÅ DataFrame.           |
| **L·ªçc ƒëi·ªÅu ki·ªán**  | `df[df['age'] > 20]`        | L·ªçc h√†ng theo Boolean mask. |
| **Theo Label**     | `df.loc[row_lbl, col_lbl]`  | Ch·ªçn theo t√™n nh√£n.         |
| **Theo V·ªã tr√≠**    | `df.iloc[row_idx, col_idx]` | Ch·ªçn theo ch·ªâ s·ªë (index).   |

### üßπ L√†m S·∫°ch & Bi·∫øn ƒê·ªïi

| H√†nh ƒë·ªông       | C√∫ ph√°p                              | Ghi ch√∫                        |
|:----------------|:-------------------------------------|:-------------------------------|
| **Check Null**  | `df.isnull().sum()`                  | ƒê·∫øm s·ªë l∆∞·ª£ng NaN m·ªói c·ªôt.      |
| **X√≥a Null**    | `df.dropna()`                        | X√≥a h√†ng c√≥ NaN.               |
| **ƒêi·ªÅn Null**   | `df.fillna(value)`                   | ƒêi·ªÅn NaN b·∫±ng gi√° tr·ªã c·ª• th·ªÉ.  |
| **S·∫Øp x·∫øp**     | `df.sort_values(by='col')`           | `ascending=False` ƒë·ªÉ gi·∫£m d·∫ßn. |
| **ƒê·ªïi t√™n c·ªôt** | `df.rename(columns={'old': 'new'})`  |                                |
| **Apply h√†m**   | `df['col'].apply(lambda x: x*2)`     | √Åp d·ª•ng h√†m cho t·ª´ng ph·∫ßn t·ª≠.  |
| **Groupby**     | `df.groupby('col')['target'].mean()` | Gom nh√≥m v√† t√≠nh to√°n.         |

---

## 2. NumPy üî¢

**M·ª•c ƒë√≠ch:** T√≠nh to√°n khoa h·ªçc, x·ª≠ l√Ω m·∫£ng ƒëa chi·ªÅu (Matrix/Vector).

### üß± Kh·ªüi T·∫°o M·∫£ng

| H√†nh ƒë·ªông    | C√∫ ph√°p                        | Ghi ch√∫                     |
|:-------------|:-------------------------------|:----------------------------|
| **Import**   | `import numpy as np`           |                             |
| **T·ª´ List**  | `np.array([1, 2, 3])`          |                             |
| **M·∫£ng 0**   | `np.zeros((2, 3))`             | M·∫£ng 2x3 to√†n s·ªë 0.         |
| **M·∫£ng 1**   | `np.ones((2, 3))`              | M·∫£ng 2x3 to√†n s·ªë 1.         |
| **Tu·∫ßn t·ª±**  | `np.arange(start, stop, step)` | Gi·ªëng range() c·ªßa Python.   |
| **Chia ƒë·ªÅu** | `np.linspace(0, 1, 5)`         | 5 ƒëi·ªÉm c√°ch ƒë·ªÅu t·ª´ 0 ƒë·∫øn 1. |

### üìê Thu·ªôc T√≠nh & Bi·∫øn ƒê·ªïi

| H√†nh ƒë·ªông      | C√∫ ph√°p                  | Ghi ch√∫                     |
|:---------------|:-------------------------|:----------------------------|
| **K√≠ch th∆∞·ªõc** | `arr.shape`              | (h√†ng, c·ªôt).                |
| **S·ªë chi·ªÅu**   | `arr.ndim`               | 1, 2, 3...                  |
| **Reshape**    | `arr.reshape(3, 2)`      | ƒê·ªïi c·∫•u tr√∫c m·∫£ng.          |
| **Transpose**  | `arr.T`                  | Chuy·ªÉn v·ªã (h√†ng th√†nh c·ªôt). |
| **N·ªëi m·∫£ng**   | `np.concatenate((a, b))` | N·ªëi a v√† b.                 |

### ‚ûï Ph√©p To√°n (Vectorization)

| H√†nh ƒë·ªông        | C√∫ ph√°p                     | Ghi ch√∫                      |
|:-----------------|:----------------------------|:-----------------------------|
| **C∆° b·∫£n**       | `arr + 5`, `arr * 2`        | Th·ª±c hi·ªán tr√™n t·ª´ng ph·∫ßn t·ª≠. |
| **Nh√¢n Ma tr·∫≠n** | `np.dot(a, b)` ho·∫∑c `a @ b` | T√≠ch v√¥ h∆∞·ªõng.               |
| **Th·ªëng k√™**     | `arr.mean()`, `arr.sum()`   | Trung b√¨nh, T·ªïng.            |
| **Max/Min**      | `arr.max()`, `arr.min()`    | Gi√° tr·ªã l·ªõn nh·∫•t/nh·ªè nh·∫•t.   |
| **Theo tr·ª•c**    | `arr.sum(axis=0)`           | 0=c·ªôt, 1=h√†ng.               |

---

## 3. Scikit-learn (Sklearn) ü§ñ

**M·ª•c ƒë√≠ch:** X√¢y d·ª±ng, hu·∫•n luy·ªán v√† ƒë√°nh gi√° m√¥ h√¨nh Machine Learning.

### ‚úÇÔ∏è Chia & X·ª≠ L√Ω D·ªØ Li·ªáu

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. Chia t·∫≠p Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Chu·∫©n h√≥a d·ªØ li·ªáu (Scaling)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit & Transform train
X_test_scaled = scaler.transform(X_test)  # Ch·ªâ Transform test
```

### üß† Quy Tr√¨nh Hu·∫•n Luy·ªán (V√≠ d·ª•: Logistic Regression)

```python

from sklearn.linear_model import LogisticRegression

# 1. Kh·ªüi t·∫°o m√¥ h√¨nh
model = LogisticRegression()

# 2. Hu·∫•n luy·ªán (Fit)
model.fit(X_train_scaled, y_train)

# 3. D·ª± ƒëo√°n (Predict)
y_pred = model.predict(X_test_scaled)
```

### üìä ƒê√°nh Gi√° M√¥ H√¨nh

```python

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# ƒê·ªô ch√≠nh x√°c
acc = accuracy_score(y_test, y_pred)

# Ma tr·∫≠n nh·∫ßm l·∫´n
cm = confusion_matrix(y_test, y_pred)

# B√°o c√°o chi ti·∫øt (Precision, Recall, F1)
print(classification_report(y_test, y_pred))
```
### ‚öôÔ∏è Tinh Ch·ªânh (Hyperparameter Tuning)

```python

from sklearn.model_selection import GridSearchCV

params = {'C': [0.1, 1, 10]}
grid = GridSearchCV(LogisticRegression(), params, cv=5)
grid.fit(X_train_scaled, y_train)

print(grid.best_params_)
```

# üìù T√†i Li·ªáu √în T·∫≠p: X·ª≠ L√Ω D·ªØ Li·ªáu & K-Fold Cross-Validation

T√†i li·ªáu n√†y t·ªïng h·ª£p code m·∫´u gi·∫£i quy·∫øt 5 y√™u c·∫ßu c·ª• th·ªÉ trong ƒë·ªÅ c∆∞∆°ng √¥n t·∫≠p c·ªßa b·∫°n.

---

## 4. Chu·∫©n H√≥a v√† S·ªë H√≥a D·ªØ Li·ªáu

### A. S·ªë h√≥a d·ªØ li·ªáu (Encoding)
Chuy·ªÉn d·ªØ li·ªáu t·ª´ d·∫°ng ch·ªØ (Categorical) sang d·∫°ng s·ªë ƒë·ªÉ m√°y h·ªçc ƒë∆∞·ª£c.

**Tr∆∞·ªùng h·ª£p 1: Label Encoder** (D√πng cho c·ªôt nh√£n m·ª•c ti√™u `y` ho·∫∑c bi·∫øn th·ª© b·∫≠c)
```python
from sklearn.preprocessing import LabelEncoder

# Gi·∫£ s·ª≠ y l√†: ['Male', 'Female', 'Male']
le = LabelEncoder()
y_encoded = le.fit_transform(y) 
# K·∫øt qu·∫£: [1, 0, 1]
```
**Tr∆∞·ªùng h·ª£p 2: One-Hot Encoder** (D√πng cho c·ªôt ƒë·∫∑c tr∆∞ng `X`)
```python
from sklearn.preprocessing import OneHotEncoder

# Gi·∫£ s·ª≠ X l√†: [['Small'], ['Medium'], ['Large']]
ohe = OneHotEncoder()
X_encoded = ohe.fit_transform(X).toarray()
# K·∫øt qu·∫£: [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
```
**Tr∆∞·ªùng h·ª£p 3: Ordinal Encoder** (D√πng cho c·ªôt ƒë·∫∑c tr∆∞ng `X`)
```python
from sklearn.preprocessing import OrdinalEncoder

# Gi·∫£ s·ª≠ X l√†: [['Low'], ['Medium'], ['High']]
oe = OrdinalEncoder()
X_encoded = oe.fit_transform(X)
# K·∫øt qu·∫£: [[0], [1], [2]]
```
### B. Chu·∫©n h√≥a d·ªØ li·ªáu (Scaling)

ƒê∆∞a d·ªØ li·ªáu v·ªÅ c√πng m·ªôt mi·ªÅn gi√° tr·ªã (th∆∞·ªùng d√πng tr∆∞·ªõc SVM, Logistic Regression).
```python

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# C√°ch 1: StandardScaler (V·ªÅ ph√¢n ph·ªëi chu·∫©n: mean=0, std=1) - Khuy√™n d√πng cho SVM/Logistic
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# C√°ch 2: MinMaxScaler (V·ªÅ kho·∫£ng [0, 1])
minmax = MinMaxScaler()
X_minmax = minmax.fit_transform(X)

from sklearn.preprocessing import Normalizer

# C√°ch 1: Normalizer (V·ªÅ kho·∫£ng [0, 1])
normalizer = Normalizer()
X_normalized = normalizer.fit_transform(X)
```
### C. Bi·∫øn ƒê·ªïi Cosin (Discrete Cosine Transform - DCT)

Trong x·ª≠ l√Ω d·ªØ li·ªáu (ƒë·∫∑c bi·ªát l√† n√©n d·ªØ li·ªáu ho·∫∑c tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng), bi·∫øn ƒë·ªïi Cosin th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu ho·∫∑c l√†m m·ªãn.
```python

from scipy.fftpack import dct
import numpy as np

# H√†m th·ª±c hi·ªán DCT tr√™n t·ª´ng h√†ng c·ªßa d·ªØ li·ªáu X
# axis=1: th·ª±c hi·ªán theo h√†ng
# type=2: lo·∫°i DCT ph·ªï bi·∫øn nh·∫•t
# norm='ortho': chu·∫©n h√≥a tr·ª±c giao
X_dct = dct(X, axis=1, type=2, norm='ortho')

# N·∫øu mu·ªën l·∫•y n th√†nh ph·∫ßn ƒë·∫ßu ti√™n (gi·∫£m chi·ªÅu)
n_components = 5
X_dct_reduced = X_dct[:, :n_components]
```


## 5. ·ª®ng d·ª•ng

### A. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng

#### 3. Chia D·ªØ Li·ªáu Train/Test Theo %

Chia d·ªØ li·ªáu th√†nh 2 ph·∫ßn c·ªë ƒë·ªãnh (V√≠ d·ª•: 70% h·ªçc, 30% thi).

```python
from sklearn.model_selection import train_test_split

# test_size=0.3: d√†nh 30% cho t·∫≠p test
# random_state=42: gi·ªØ c·ªë ƒë·ªãnh c√°ch chia ƒë·ªÉ k·∫øt qu·∫£ kh√¥ng ƒë·ªïi m·ªói l·∫ßn ch·∫°y
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42
)
```

#### 4 & 5. Quy Tr√¨nh K-Fold Cross-Validation (Tr·ªçng T√¢m)

ƒê√¢y l√† ph·∫ßn quan tr·ªçng nh·∫•t: Chia d·ªØ li·ªáu th√†nh K ph·∫ßn, l·∫ßn l∆∞·ª£t d√πng 1 ph·∫ßn ƒë·ªÉ test v√† K-1 ph·∫ßn ƒë·ªÉ train, sau ƒë√≥ t√≠nh trung b√¨nh c√°c ch·ªâ s·ªë ƒë√°nh gi√°.

##### C√°c th∆∞ vi·ªán c·∫ßn thi·∫øt

```python
from sklearn.model_selection import KFold
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
```

##### Code m·∫´u th·ª±c hi·ªán v√≤ng l·∫∑p K-Fold

ƒêo·∫°n code n√†y √°p d·ª•ng cho c·∫£ SVM v√† Logistic Regression.

```python
# 1. Kh·ªüi t·∫°o K-Fold
k = 5
kf = KFold(n_splits=k, shuffle=True, random_state=42)

# 2. Ch·ªçn m√¥ h√¨nh (B·ªè comment m√¥ h√¨nh b·∫°n mu·ªën d√πng)
model = SVC(kernel='linear')               # SVM
# model = LogisticRegression(max_iter=1000)  # Logistic Regression

# 3. T·∫°o danh s√°ch l∆∞u k·∫øt qu·∫£
acc_scores = []
pre_scores = []
rec_scores = []

print(f"B·∫Øt ƒë·∫ßu ch·∫°y {k}-Fold Cross-Validation...")

# 4. V√≤ng l·∫∑p K-Fold
# L∆∞u √Ω: X v√† y ph·∫£i l√† d·∫°ng numpy array. N·∫øu l√† DataFrame, d√πng X.values, y.values
X_arr = np.array(X)
y_arr = np.array(y)

for fold_idx, (train_index, test_index) in enumerate(kf.split(X_arr)):
    # A. L·∫•y d·ªØ li·ªáu theo index c·ªßa fold hi·ªán t·∫°i
    X_train_fold, X_test_fold = X_arr[train_index], X_arr[test_index]
    y_train_fold, y_test_fold = y_arr[train_index], y_arr[test_index]

    # B. Hu·∫•n luy·ªán m√¥ h√¨nh
    model.fit(X_train_fold, y_train_fold)

    # C. D·ª± ƒëo√°n
    y_pred_fold = model.predict(X_test_fold)

    # D. T√≠nh c√°c ch·ªâ s·ªë
    # average='macro' ho·∫∑c 'weighted' n·∫øu b√†i to√°n ph√¢n lo·∫°i nhi·ªÅu l·ªõp (multi-class)
    # average='binary' n·∫øu ch·ªâ c√≥ 2 l·ªõp (0 v√† 1)
    acc = accuracy_score(y_test_fold, y_pred_fold)
    pre = precision_score(y_test_fold, y_pred_fold, average='macro', zero_division=0)
    rec = recall_score(y_test_fold, y_pred_fold, average='macro', zero_division=0)

    # L∆∞u v√†o danh s√°ch
    acc_scores.append(acc)
    pre_scores.append(pre)
    rec_scores.append(rec)

    print(f"Fold {fold_idx+1}: Accuracy={acc:.4f}, Precision={pre:.4f}, Recall={rec:.4f}")

# 5. T√≠nh trung b√¨nh k·∫øt qu·∫£ cu·ªëi c√πng
print("-" * 30)
print(f"K·∫æT QU·∫¢ TRUNG B√åNH ({k} folds):")
print(f"Accuracy : {np.mean(acc_scores):.4f}")
print(f"Precision: {np.mean(pre_scores):.4f}")
print(f"Recall   : {np.mean(rec_scores):.4f}")
```

##### Gi·∫£i th√≠ch c√°c tham s·ªë quan tr·ªçng trong metrics:

- `average='binary'`: D√πng cho b√†i to√°n 2 l·ªõp (VD: ƒê√∫ng/Sai).
- `average='macro'`: D√πng cho b√†i to√°n nhi·ªÅu l·ªõp (VD: Hoa A, Hoa B, Hoa C), t√≠nh trung b√¨nh c√°c l·ªõp kh√¥ng tr·ªçng s·ªë.
- `zero_division=0`: Tr√°nh l·ªói chia cho 0 n·∫øu m√¥ h√¨nh kh√¥ng d·ª± ƒëo√°n ƒë∆∞·ª£c l·ªõp n√†o ƒë√≥.